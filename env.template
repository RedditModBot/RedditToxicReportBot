# =========================
# Reddit credentials
# =========================
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USERNAME=YourBotUser
REDDIT_PASSWORD=your_bot_password
REDDIT_USER_AGENT=toxic-report-bot/2.0 by u/YourBotUser

# =========================
# Subreddits to monitor (comma-separated)
# =========================
SUBREDDITS=UFOs

# =========================
# LLM Configuration (Groq - FREE!)
# =========================
# Get your FREE API key at https://console.groq.com/
GROQ_API_KEY=gsk_xxxxx

# Optional: x.ai Grok API (paid, but cheap)
# Get your API key at https://console.x.ai/
# Enables grok-* models as primary or fallback
# XAI_API_KEY=xai-xxxxx

# Primary model - the bot will try this first
# Use "grok-*" models if you have XAI_API_KEY set
LLM_MODEL=grok-4-0709

# Fallback chain - comma-separated list of models to try if primary fails
# The bot will try each model in order until one succeeds
# Recommended: Use reasoning models for better judgment on gray areas
LLM_FALLBACK_CHAIN=openai/gpt-oss-120b,openai/gpt-oss-20b,qwen/qwen3-32b

# =========================
# Model Options & Reasoning Support
# =========================
#
# REASONING MODELS (recommended for moderation - think before answering):
#
# X.AI GROK (paid) - requires XAI_API_KEY:
# | Model           | Reasoning | Cost (per M tokens)   | Notes                    |
# |-----------------|-----------|----------------------|--------------------------|
# | grok-4-0709     | ✅ Always  | ~$2-5 in / ~$10 out  | Best quality, always reasons |
# | grok-3-mini     | ✅ Config  | Cheaper              | Use XAI_REASONING_EFFORT |
# | grok-3          | ❌ None    | $3 in / $15 out      | No reasoning support     |
#
# GROQ FREE TIER - Reasoning models:
# | Model                | Reasoning | RPD  | TPD   | Notes                      |
# |----------------------|-----------|------|-------|----------------------------|
# | openai/gpt-oss-120b  | ✅ Config  | 1K   | 200K  | Best free reasoning model  |
# | openai/gpt-oss-20b   | ✅ Config  | 1K   | 200K  | Smaller, still reasons     |
# | qwen/qwen3-32b       | ✅ Always  | 1K   | 500K  | Always reasons, good limits|
#
# GROQ FREE TIER - Non-reasoning models (not recommended for primary):
# | Model                                         | RPD   | TPD   | Notes                    |
# |-----------------------------------------------|-------|-------|--------------------------|
# | groq/compound                                 | 250   | None  | Smart routing, no reasoning |
# | llama-3.3-70b-versatile                       | 1K    | 100K  | Good quality, no reasoning |
# | meta-llama/llama-4-scout-17b-16e-instruct     | 1K    | 500K  | No reasoning             |
# | llama-3.1-8b-instant                          | 14.4K | 500K  | Fast fallback, no reasoning |

# Max requests per minute to Groq (prevents rate limiting)
# Default 2 = one request every 30 seconds max
LLM_REQUESTS_PER_MINUTE=2

# =========================
# Reasoning Effort Settings
# =========================
# These control how hard reasoning models "think" before answering.
# Higher = better accuracy on gray areas, but uses more tokens.

# x.ai reasoning effort (ONLY for grok-3-mini, grok-4 always reasons)
# Options: "low", "medium", "high"
# Not needed if using grok-4 (it always reasons at full capacity)
#XAI_REASONING_EFFORT=high

# Groq reasoning effort (for openai/gpt-oss-20b and openai/gpt-oss-120b)
# Options: "low", "medium", "high"
# qwen3 and deepseek-r1 always reason (no effort setting)
GROQ_REASONING_EFFORT=high

# Legacy - daily limit before switching (less relevant with fallback chain)
LLM_DAILY_LIMIT=14000

# =========================
# Detoxify Pre-filter
# =========================
# Detoxify runs locally to decide what gets sent to the LLM.
# Lower = more comments sent to LLM (more API calls, fewer misses)
# Higher = fewer comments sent to LLM (fewer API calls, might miss some)

# Detoxify model: "original" (more aggressive) or "unbiased" (more conservative)
DETOXIFY_MODEL=original

# Should Detoxify trigger LLM review on its own?
# If false, Detoxify only provides scores for context but won't cause escalation
# OpenAI/Perspective can still trigger escalation based on their modes
# Useful if Detoxify has too many false positives for your use case
DETOXIFY_CAN_ESCALATE=true

# =========================
# OpenAI Moderation API (optional, FREE)
# =========================
# Supplement to Detoxify using OpenAI's free Moderation API
# Detects: hate, harassment, self-harm, sexual, violence
# Requires OpenAI API key (same as for other OpenAI services)
#
# OPENAI_API_KEY=sk-xxxxx  (set above or here)
OPENAI_MODERATION_ENABLED=false

# Base threshold (0.0-1.0) - individual categories have their own thresholds
OPENAI_MODERATION_THRESHOLD=0.50

# Rate limit (requests per minute) to avoid 429 errors
OPENAI_MODERATION_RPM=10

# Mode: when to call the API
# - "all"     = Run on EVERY comment (uses more API calls)
# - "confirm" = Only if Detoxify triggers (recommended, saves API calls)
# - "only"    = Skip Detoxify, use only this API
OPENAI_MODERATION_MODE=confirm

# =========================
# Google Perspective API (optional, FREE)
# =========================
# Supplement to Detoxify using Google's Perspective API
# Detects: toxicity, severe_toxicity, identity_attack, insult, profanity, threat
# Get API key at: https://developers.google.com/codelabs/setup-perspective-api
#
PERSPECTIVE_API_KEY=
PERSPECTIVE_ENABLED=false

# Base threshold (0.0-1.0) - individual categories have their own thresholds
PERSPECTIVE_THRESHOLD=0.70

# Rate limit (requests per minute)
PERSPECTIVE_RPM=60

# Mode: when to call the API
# - "all"     = Run on EVERY comment (uses more API calls)
# - "confirm" = Only if Detoxify triggers (recommended, saves API calls)
# - "only"    = Skip Detoxify, use only this API
PERSPECTIVE_MODE=confirm

# =========================
# Detection Thresholds
# =========================
# Per-label thresholds for Detoxify scores (0.0 to 1.0)
# If ANY score exceeds its threshold, comment is sent to AI for review
# Lower = more sensitive (catches more, but more API calls)
# Higher = less sensitive (misses some, but fewer API calls)

# Main pre-filter threshold (recommended: 0.5)
# Comments below this skip Detoxify label checks entirely
DETOXIFY_THRESHOLD=0.5

# High-priority categories (keep low to catch threats)
THRESHOLD_THREAT=0.15
THRESHOLD_SEVERE_TOXICITY=0.20
THRESHOLD_IDENTITY_ATTACK=0.25

# Insults (different thresholds for directed vs not directed at user)
# "Directed" = contains "you", "your", "OP", or is a reply
THRESHOLD_INSULT_DIRECTED=0.40
THRESHOLD_INSULT_NOT_DIRECTED=0.65

# General toxicity (also has directed/not directed)
THRESHOLD_TOXICITY_DIRECTED=0.50
THRESHOLD_TOXICITY_NOT_DIRECTED=0.65

# Obscene language (keep high - profanity alone isn't necessarily toxic)
THRESHOLD_OBSCENE=0.90

# Borderline skip notification threshold
# Comments scoring above this but below other thresholds are logged as "borderline"
THRESHOLD_BORDERLINE=0.35

# =========================
# Moderation Guidelines
# =========================
# Path to your custom moderation guidelines file
# If not found, uses built-in defaults
MODERATION_GUIDELINES_FILE=moderation_guidelines.txt

# Or set guidelines directly (overrides file):
# MODERATION_GUIDELINES=Your guidelines here...

# =========================
# Reporting behavior
# =========================
# moderator = file a moderator report (requires mod permissions)
# user = file a standard user report
REPORT_AS=moderator

# Optional rule bucket for Reddit's report UI (e.g., "Harassment", "Hate")
REPORT_RULE_BUCKET=

# Master switch for Reddit reporting
ENABLE_REDDIT_REPORTS=true

# If true, log verdicts but DO NOT file reports (good for testing)
DRY_RUN=true

# =========================
# Auto-Remove (optional)
# =========================
# Automatically remove comments when multiple ML models agree they're toxic.
# Comment goes to mod queue for human review (can be approved if wrong).
# Requires LLM verdict of REPORT + consensus from specified models.
#
# IMPORTANT: Start conservative! Set high thresholds and test before enabling.

# Master switch for auto-remove
AUTO_REMOVE_ENABLED=false

# Which models must agree for auto-remove (comma-separated)
# Options: detoxify, openai, perspective
# Example: "openai,perspective" = only these two must pass
AUTO_REMOVE_REQUIRE_MODELS=openai,perspective

# Minimum number of models that must pass their threshold
# Set to number of models in REQUIRE_MODELS for "all must agree"
# Set lower for "X of Y must agree" (e.g., 2 of 3)
AUTO_REMOVE_MIN_CONSENSUS=2

# Minimum scores for each model to pass (only checked if in REQUIRE_MODELS)
AUTO_REMOVE_DETOXIFY_MIN=0.70
AUTO_REMOVE_OPENAI_MIN=0.70
AUTO_REMOVE_PERSPECTIVE_MIN=0.70

# Also auto-remove on pattern matches? (slurs, threats, self-harm)
# These are high-confidence but still go through LLM review first
AUTO_REMOVE_ON_PATTERN_MATCH=false

# =========================
# Discord (optional)
# =========================
# Set up Discord notifications for:
# - Real-time alerts when comments are reported
# - Daily stats summary (posted at midnight UTC)
# - False positive alerts (when reported comments weren't removed)
#
# To create a webhook:
# 1. Go to your Discord server
# 2. Edit a channel → Integrations → Webhooks → New Webhook
# 3. Copy the webhook URL and paste below
#
ENABLE_DISCORD=false
DISCORD_WEBHOOK=https://discord.com/api/webhooks/your_webhook_id/your_webhook_token

# =========================
# Logging
# =========================
LOG_LEVEL=INFO
